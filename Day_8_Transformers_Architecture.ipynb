{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wDrl-qm7hawN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Architecture for Text Classification"
      ],
      "metadata": {
        "id": "eyAXtU0I952Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAtention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAtention, self).__init__()\n",
        "    assert d_model % num_heads == 0,  \"d_models must be divisible by num_heads\"\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    # Linears layers for query, key, value and output\n",
        "\n",
        "    self.W_q = nn.Linear(d_model, d_model)\n",
        "    self.W_k = nn.Linear(d_model, d_model)\n",
        "    self.W_v = nn.Linear(d_model, d_model)\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def scale_dot_product(self, Q, K, V, mask=None):\n",
        "    # Compute attention score\n",
        "    attn_score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "    # Apply mask if provide (useful for decoder transfer)\n",
        "    if mask is not None:\n",
        "      attn_score = attn_score.masked_fill(mask==0, float('-inf'))\n",
        "\n",
        "    # Softmax probabilties of attention score\n",
        "    attn_probs = F.softmax(attn_score, dim=-1)\n",
        "    output = torch.matmul(attn_probs, V)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    batch_size, seq_length, d_model = x.size()\n",
        "    return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "    batch_size,_, seq_length, _ = x.size()\n",
        "    return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "\n",
        "    # Linear projections\n",
        "    K = self.split_heads(self.W_k(K))\n",
        "    Q = self.split_heads(self.W_q(Q))\n",
        "    V = self.split_heads(self.W_v(V))\n",
        "\n",
        "    attn_output = self.scale_dot_product(Q, K, V, mask)\n",
        "    # Combine heads and projection\n",
        "    out = self.w_o(self.combine_heads(attn_output))\n",
        "    return out\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_seq_length=5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    # Create positional Encoding\n",
        "    pe = torch.zeros(max_seq_length, d_model)\n",
        "    position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "    super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "    self.self_attn = MultiHeadAtention(d_model, num_heads)\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(d_model, d_ff),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(d_ff, d_model)\n",
        "    )\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "\n",
        "    # self-attention on sub-layer\n",
        "    attn_output = self.self_attn(x, x, x, mask)\n",
        "    x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "    #feed-forward sub-layer\n",
        "    ff_output = self.feed_forward(x)\n",
        "    x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "    return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      d_model=512,\n",
        "      num_heads=8,\n",
        "      num_layers=6,\n",
        "      d_ff=2048,\n",
        "      max_seq_length=5000,\n",
        "      dropout=0.1,\n",
        "      num_classes=10):\n",
        "\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    # Embedding_layer\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    # Positional Encodding\n",
        "    self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "    # Encoder Layers\n",
        "\n",
        "    self.encoder_layers = nn.ModuleList([\n",
        "        TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "    # Dropout and classifier layer\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    # Embedding and positional embedding\n",
        "    x = self.embedding(x)\n",
        "    x = self.positional_encoding(x)\n",
        "\n",
        "    # Encoder stack\n",
        "\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x, mask)\n",
        "\n",
        "    # Global Avg pooling\n",
        "\n",
        "    x = x.mean(dim=1)\n",
        "\n",
        "    x = self.classifier(self.dropout(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "def create_transformers_model(vocab_size=10000, num_classes=10):\n",
        "  model = Transformer(\n",
        "      vocab_size=vocab_size,\n",
        "      d_model=512,\n",
        "      num_heads=8,\n",
        "      num_layers=6,\n",
        "      d_ff = 2048,\n",
        "      num_classes=num_classes\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "UXDFS8n1hi0R"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo():\n",
        "  # Hyperparameter\n",
        "\n",
        "  batch_size = 32\n",
        "  seq_length = 100\n",
        "  vocab_size = 10000\n",
        "  num_classes = 10\n",
        "\n",
        "  model = create_transformers_model(vocab_size=vocab_size, num_classes=num_classes)\n",
        "\n",
        "  # create random input\n",
        "  input = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
        "\n",
        "  output = model(input)\n",
        "\n",
        "  print(f\"Input shape: \", input.shape)\n",
        "  print(f\"Output shape: \", output.shape)\n",
        "\n",
        "  return model\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ijFYxLprz8jy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuC6PlmC09_8",
        "outputId": "53cf2b1d-d1d2-4555-ddc7-b718f51f0d87"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  torch.Size([32, 100])\n",
            "Output shape:  torch.Size([32, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo --quiet"
      ],
      "metadata": {
        "id": "k6d4BO_X2ZDr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH9UiLBR2c2r",
        "outputId": "5f71ec49-65da-421b-f6f9-eb4143f4ef91"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "Transformer                              --\n",
              "├─Embedding: 1-1                         5,120,000\n",
              "├─PositionalEncoding: 1-2                --\n",
              "├─ModuleList: 1-3                        --\n",
              "│    └─TransformerEncoderLayer: 2-1      --\n",
              "│    │    └─MultiHeadAtention: 3-1       1,050,624\n",
              "│    │    └─Sequential: 3-2              2,099,712\n",
              "│    │    └─LayerNorm: 3-3               1,024\n",
              "│    │    └─LayerNorm: 3-4               1,024\n",
              "│    │    └─Dropout: 3-5                 --\n",
              "│    └─TransformerEncoderLayer: 2-2      --\n",
              "│    │    └─MultiHeadAtention: 3-6       1,050,624\n",
              "│    │    └─Sequential: 3-7              2,099,712\n",
              "│    │    └─LayerNorm: 3-8               1,024\n",
              "│    │    └─LayerNorm: 3-9               1,024\n",
              "│    │    └─Dropout: 3-10                --\n",
              "│    └─TransformerEncoderLayer: 2-3      --\n",
              "│    │    └─MultiHeadAtention: 3-11      1,050,624\n",
              "│    │    └─Sequential: 3-12             2,099,712\n",
              "│    │    └─LayerNorm: 3-13              1,024\n",
              "│    │    └─LayerNorm: 3-14              1,024\n",
              "│    │    └─Dropout: 3-15                --\n",
              "│    └─TransformerEncoderLayer: 2-4      --\n",
              "│    │    └─MultiHeadAtention: 3-16      1,050,624\n",
              "│    │    └─Sequential: 3-17             2,099,712\n",
              "│    │    └─LayerNorm: 3-18              1,024\n",
              "│    │    └─LayerNorm: 3-19              1,024\n",
              "│    │    └─Dropout: 3-20                --\n",
              "│    └─TransformerEncoderLayer: 2-5      --\n",
              "│    │    └─MultiHeadAtention: 3-21      1,050,624\n",
              "│    │    └─Sequential: 3-22             2,099,712\n",
              "│    │    └─LayerNorm: 3-23              1,024\n",
              "│    │    └─LayerNorm: 3-24              1,024\n",
              "│    │    └─Dropout: 3-25                --\n",
              "│    └─TransformerEncoderLayer: 2-6      --\n",
              "│    │    └─MultiHeadAtention: 3-26      1,050,624\n",
              "│    │    └─Sequential: 3-27             2,099,712\n",
              "│    │    └─LayerNorm: 3-28              1,024\n",
              "│    │    └─LayerNorm: 3-29              1,024\n",
              "│    │    └─Dropout: 3-30                --\n",
              "├─Dropout: 1-4                           --\n",
              "├─Linear: 1-5                            5,130\n",
              "=================================================================\n",
              "Total params: 24,039,434\n",
              "Trainable params: 24,039,434\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Architecture for Next word prediction"
      ],
      "metadata": {
        "id": "ZC50KImM-ANo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAtention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAtention, self).__init__()\n",
        "    assert d_model % num_heads == 0,  \"d_models must be divisible by num_heads\"\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    # Linears layers for query, key, value and output\n",
        "\n",
        "    self.W_q = nn.Linear(d_model, d_model)\n",
        "    self.W_k = nn.Linear(d_model, d_model)\n",
        "    self.W_v = nn.Linear(d_model, d_model)\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def scale_dot_product(self, Q, K, V, mask=None):\n",
        "    # Compute attention score\n",
        "    attn_score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "    # Apply mask if provide (useful for decoder transfer)\n",
        "    if mask is not None:\n",
        "      if mask.dim() == 2:\n",
        "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
        "      elif mask.dim() == 3:\n",
        "        mask = mask.unsqueeze(1)\n",
        "      attn_score = attn_score.masked_fill(mask==0, float('-inf'))\n",
        "\n",
        "    # Softmax probabilties of attention score\n",
        "    attn_probs = F.softmax(attn_score, dim=-1)\n",
        "    output = torch.matmul(attn_probs, V)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    batch_size, seq_length, d_model = x.size()\n",
        "    return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "    batch_size,_, seq_length, _ = x.size()\n",
        "    return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "\n",
        "    # Linear projections\n",
        "    K = self.split_heads(self.W_k(K))\n",
        "    Q = self.split_heads(self.W_q(Q))\n",
        "    V = self.split_heads(self.W_v(V))\n",
        "\n",
        "    attn_output = self.scale_dot_product(Q, K, V, mask)\n",
        "    # Combine heads and projection\n",
        "    out = self.w_o(self.combine_heads(attn_output))\n",
        "    return out\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_seq_length=5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    # Create positional Encoding\n",
        "    pe = torch.zeros(max_seq_length, d_model)\n",
        "    position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "    super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "    self.self_attn = MultiHeadAtention(d_model, num_heads)\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(d_model, d_ff),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(d_ff, d_model)\n",
        "    )\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "\n",
        "    # self-attention on sub-layer\n",
        "    attn_output = self.self_attn(x, x, x, mask)\n",
        "    x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "    #feed-forward sub-layer\n",
        "    ff_output = self.feed_forward(x)\n",
        "    x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "    return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      d_model=512,\n",
        "      num_heads=8,\n",
        "      num_layers=6,\n",
        "      d_ff=2048,\n",
        "      max_seq_length=5000,\n",
        "      dropout=0.1):\n",
        "\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    # Embedding_layer\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    # Positional Encodding\n",
        "    self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "    # Encoder Layers\n",
        "\n",
        "    self.encoder_layers = nn.ModuleList([\n",
        "        TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "    # Dropout and classifier layer\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_length = max_seq_length\n",
        "\n",
        "  def generate_square_subsequent_mask(self, sz):\n",
        "    \"\"\"Generate a square mask to prevent attending future tokens.\"\"\"\n",
        "    mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
        "    return mask == 0\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "\n",
        "    # create attention mask\n",
        "    mask = self.generate_square_subsequent_mask(x.size(1)).to(x.device)\n",
        "\n",
        "    # Embedding and positional embedding\n",
        "    x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "    x = self.positional_encoding(x)\n",
        "\n",
        "    # Encoder stack\n",
        "\n",
        "    x = x.transpose(0, 1)\n",
        "\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x, mask)\n",
        "\n",
        "    x = self.fc_out(self.dropout(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "def create_transformers_model(vocab_size=10000):\n",
        "  model = Transformer(\n",
        "      vocab_size=vocab_size,\n",
        "      d_model=512,\n",
        "      num_heads=8,\n",
        "      num_layers=6,\n",
        "      d_ff = 2048\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Yt6ATxfS9_eb"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_transformers_model()"
      ],
      "metadata": {
        "id": "qUCyP1ieCb75"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-POoXpOCidg",
        "outputId": "21ae1d74-9168-4293-e515-98ce2c415177"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "Transformer                              --\n",
              "├─Embedding: 1-1                         5,120,000\n",
              "├─PositionalEncoding: 1-2                --\n",
              "├─ModuleList: 1-3                        --\n",
              "│    └─TransformerEncoderLayer: 2-1      --\n",
              "│    │    └─MultiHeadAtention: 3-1       1,050,624\n",
              "│    │    └─Sequential: 3-2              2,099,712\n",
              "│    │    └─LayerNorm: 3-3               1,024\n",
              "│    │    └─LayerNorm: 3-4               1,024\n",
              "│    │    └─Dropout: 3-5                 --\n",
              "│    └─TransformerEncoderLayer: 2-2      --\n",
              "│    │    └─MultiHeadAtention: 3-6       1,050,624\n",
              "│    │    └─Sequential: 3-7              2,099,712\n",
              "│    │    └─LayerNorm: 3-8               1,024\n",
              "│    │    └─LayerNorm: 3-9               1,024\n",
              "│    │    └─Dropout: 3-10                --\n",
              "│    └─TransformerEncoderLayer: 2-3      --\n",
              "│    │    └─MultiHeadAtention: 3-11      1,050,624\n",
              "│    │    └─Sequential: 3-12             2,099,712\n",
              "│    │    └─LayerNorm: 3-13              1,024\n",
              "│    │    └─LayerNorm: 3-14              1,024\n",
              "│    │    └─Dropout: 3-15                --\n",
              "│    └─TransformerEncoderLayer: 2-4      --\n",
              "│    │    └─MultiHeadAtention: 3-16      1,050,624\n",
              "│    │    └─Sequential: 3-17             2,099,712\n",
              "│    │    └─LayerNorm: 3-18              1,024\n",
              "│    │    └─LayerNorm: 3-19              1,024\n",
              "│    │    └─Dropout: 3-20                --\n",
              "│    └─TransformerEncoderLayer: 2-5      --\n",
              "│    │    └─MultiHeadAtention: 3-21      1,050,624\n",
              "│    │    └─Sequential: 3-22             2,099,712\n",
              "│    │    └─LayerNorm: 3-23              1,024\n",
              "│    │    └─LayerNorm: 3-24              1,024\n",
              "│    │    └─Dropout: 3-25                --\n",
              "│    └─TransformerEncoderLayer: 2-6      --\n",
              "│    │    └─MultiHeadAtention: 3-26      1,050,624\n",
              "│    │    └─Sequential: 3-27             2,099,712\n",
              "│    │    └─LayerNorm: 3-28              1,024\n",
              "│    │    └─LayerNorm: 3-29              1,024\n",
              "│    │    └─Dropout: 3-30                --\n",
              "├─Dropout: 1-4                           --\n",
              "├─Linear: 1-5                            5,130,000\n",
              "=================================================================\n",
              "Total params: 29,164,304\n",
              "Trainable params: 29,164,304\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randint(0, 10000, (16, 16))\n",
        "print(dummy_input.shape)\n",
        "torch.argmax(F.softmax(model(dummy_input), dim=2), dim=2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2XEma2PDDjG",
        "outputId": "3197b86c-cb32-48c1-9c5d-9d86c48ab071"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 16])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    }
  ]
}